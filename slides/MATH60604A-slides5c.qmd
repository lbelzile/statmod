---
title: "Statistical modelling"
author: "Léo Belzile, HEC Montréal"
subtitle: "05. Linear models (coefficient of determination)"
date: today
date-format: YYYY
eval: true
cache: true
echo: true
knitr.digits.signif: true
standalone: true
bibliography: MATH60604A.bib
format:
  revealjs:
    slide-number: true
    preview-links: auto
    code-block-height: 750px
    theme: [simple, hecmontreal.scss]
    title-slide-attributes:
      data-background-color: "#002855"
    logo: "fig/logo_hec_montreal_bleu_web.png"
    width: 1600
    height: 900
---


```{r}
#| eval: true
#| include: false
#| cache: false
hecbleu <- c("#002855")
fcols <- c(gris = "#888b8d",
           bleu = "#0072ce",
           aqua = "#00aec7",
           vert = "#26d07c",
           rouge = "#ff585d",
           rose = "#eb6fbd",
           jaune = "#f3d03e")
pcols <- c(gris = "#d9d9d6",
           bleu = "#92c1e9",
           agua = "#88dbdf",
           vert = "#8fe2b0",
           rouge = "#ffb1bb",
           rose = "#eab8e4",
           jaune = "#f2f0a1")
library(ggplot2)
theme_set(theme_classic())
library(patchwork)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(digits = 2, width = 75)
```



## Pearson's linear correlation coefficient

The Pearson correlation coefficient quantifies the strength of the linear relationship between two random variables $X$ and $Y$. 
\begin{align*}
\rho= \mathsf{cor}(X, Y) =  \frac{{\mathsf{Co}}(X,Y)}{\sqrt{{\mathsf{Va}}(X){\mathsf{Va}}(Y)}}. 
\end{align*}

- The sample correlation $\rho \in [-1, 1]$.
- $|\rho|=1$ if and only if the $n$ observations fall exactly on a line.
- The larger $|\rho|$, the less scattered the points are.

## Properties of Pearson's linear correlation coefficient

The sign determines the orientation of the slope.

- If $\rho>0$, the variables are positively associated, meaning $Y$ increases on average with $X$.
- If $\rho <0$, the association is negative and $Y$ decreases on average with $X$.

```{r}
#| eval: true
#| echo: false
#| fig-width: 12
#| fig-height: 3
#| out-width: '100%'
#| label: fig-scatterplot-corr
#| fig-cap: "Scatterplots of observations with correlations of $0.1$, $0.5$, $-0.75$ and $0.95$ from $A$ to $D$." 
set.seed(1234)
xdat <- rbind(
  MASS::mvrnorm(n = 100, mu = c(0,0), Sigma = cbind(c(1, 0.1), c(0.1,1))),
  MASS::mvrnorm(n = 100, mu = c(0,0), Sigma = cbind(c(1, 0.5), c(0.5,1))),
  MASS::mvrnorm(n = 100, mu = c(0,0), Sigma = cbind(c(1, -0.75), c(-0.75,1))),
  MASS::mvrnorm(n = 100, mu = c(0,0), Sigma = cbind(c(1, 0.95), c(0.95,1)))
)
colnames(xdat) <- c("x","y")
data.frame(dataset = factor(rep(LETTERS[1:4], each = 100)),
           xdat) |>
ggplot(mapping = aes(x = x, y = y, group = dataset)) +
  geom_point() +
  geom_smooth(se = FALSE, formula = y ~ x, method = "lm", col = "grey") + 
  facet_wrap(~dataset, nrow = 1, ncol = 4, scales = "free") +
  labs(x = "", y = "")

```


## Correlation and independence

- Independent variables are uncorrelated (not the other way around).
- A correlation of zero only implies that there is no *linear* dependence between two variables.

```{r}
#| eval: true
#| echo: false
#| out-width: '100%'
#| fig-width: 12
#| fig-height: 3
#| label: fig-datasaurus
#| fig-cap: "Four datasets with dependent data having identical summary statistics and a linear correlation of -0.06."
datasauRus::datasaurus_dozen |>
  dplyr::filter(dataset %in% c("dino","bullseye","star","x_shape")) |>
  ggplot(mapping = aes(x = x, y = y, group = dataset)) +
  geom_point() + 
  geom_smooth(se = FALSE, formula = y ~ x, method = "lm", col = "grey") + 
  facet_wrap(~dataset, nrow = 1, ncol = 4) + labs(x = "", y = "")
```



## Sum of squares decomposition

Suppose that we do not use any explanatory variable (i.e., the intercept-only model). In this case, the fitted value for $Y$ is the overall mean and the sum of squared centered observations
\begin{align*}
\mathsf{SS}_c=\sum_{i=1}^n (Y_i-\overline{Y})^2                                              
\end{align*}
where $\overline{Y}$ represents the intercept-only fitted value.

When we include the $p$ regressors,  we get rather
\begin{align*}
\mathsf{SS}_e=\sum_{i=1}^n (Y_i-\widehat{Y}_i)^2 
\end{align*}
The $\mathsf{SS}_e$ is non-increasing when we include more variables.

## Percentage of variance

Consider the sum of squared residuals for two models:

- $\mathsf{SS}_c$ is for the intercept-only model
- $\mathsf{SS}_e$ for the linear regression with model matrix $\mathbf{X}$. 

Consequently, $\mathsf{SS}_c-\mathsf{SS}_e$ is the reduction of the error associated with including $\mathbf{X}$ in the model
\begin{align*}
R^2=\frac{\mathsf{SS}_c-\mathsf{SS}_e}{\mathsf{SS}_c}                                                     
\end{align*}
This gives the proportion of the variability in $\boldsymbol{Y}$ explained by $\mathbf{X}$.
 

## Coefficient of determination

We can show that the coefficient of determination is the square of Pearson's linear correlation between the response $\boldsymbol{y}$ and the fitted values $\widehat{\boldsymbol{y}}$,
$$R^2 = \mathsf{cor}^2(\boldsymbol{y}, \widehat{\boldsymbol{y}}).$$

  

```{r}
#| eval: true
#| echo: true
data(college, package = "hecstatmod")
mod <- lm(salary ~ sex + field + rank + service, data = college)
summary(mod)$r.squared # R-squared from output
y <- college$salary # response vector
yhat <- fitted(mod) # fitted value
cor(y, yhat)^2 
```

- $R^2$ always takes a value between $0$ and $1$.
- $R^2$ is not a goodness-of-fit criterion: the coefficient is non-decreasing so the more explanatories are added to $\mathbf{X}$, the higher the $R^2$.
