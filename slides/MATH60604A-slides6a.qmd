---
title: "Statistical modelling"
author: "Léo Belzile, HEC Montréal"
subtitle: "06. Linear models"
date: today
date-format: YYYY
eval: true
cache: false
echo: false
fig-align: 'center'
out-width: '100%'
standalone: true
bibliography: MATH60604A.bib
format:
  revealjs:
    slide-number: true
    preview-links: auto
    code-block-height: 750px
    theme: [simple, hecmontreal.scss]
    title-slide-attributes:
      data-background-color: "#002855"
    logo: "fig/logo_hec_montreal_bleu_web.png"
    width: 1600
    height: 900
---


```{r}
#| eval: true
#| include: false
#| cache: false
hecblue <- c("#002855")

fcols <- c(gris = "#888b8d",
           bleu = "#0072ce",
           aqua = "#00aec7",
           vert = "#26d07c",
           rouge = "#ff585d",
           rose = "#eb6fbd",
           jaune = "#f3d03e")
pcols <- c(gris = "#d9d9d6",
           bleu = "#92c1e9",
           agua = "#88dbdf",
           vert = "#8fe2b0",
           rouge = "#ffb1bb",
           rose = "#eab8e4",
           jaune = "#f2f0a1")
library(ggplot2)
library(knitr)
theme_set(theme_classic())
library(patchwork)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(scipen=1, digits=2, width = 75)
options(knitr.kable.NA = '')
library(kableExtra)
library(car)
```

## Model assumptions

There are four main assumptions of the linear model specification $$Y_i \mid \mathbf{x}_i \sim \mathsf{normal}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2).$$

- linearity and additivity: the mean of $Y_i \mid \mathbf{x}_i$ is $\beta_0 + \beta_1x_{i1} + \cdots + \beta_p x_{ip}$.
- homoscedasticity: the error variance $\sigma^2$ is constant
- independence of the errors/observations conditional on covariates
- normality

## Read the fine prints

Our strategy is to create graphical diagnostic tools or perform hypothesis tests to ensure that there is no gross violation of the model underlying assumptions.

- When we perform an hypothesis test, we merely fail to reject the null hypothesis, either because the latter is true or else due to lack of evidence. 
- The same goes for checking the validity of model assumptions.
- Beware of over-interpreting diagnostic plots: the human eye is very good at finding spurious patterns...


## Assumption 1 - mean model specification


The mean is $$\mathsf{E}(Y_i \mid \mathbf{x}_i)=\beta_0 + \beta_1x_{i1} + \cdots + \beta_p x_{ip}.$$

Implicitly,

- All interactions are included.
- There are no omitted explanatories from the model,
- The relationship between $Y_i$ and $X_j$ is linear.
- The effect is additive.

## Diagnostic plots for linearity

Use ordinary residuals $\boldsymbol{e}$, which are uncorrelated with fitted values $\widehat{\boldsymbol{y}}$ and explanatory variables (i.e., columns of $\mathbf{X}$).

- Plots of residuals $\boldsymbol{e}$ against fitted values $\widehat{\boldsymbol{y}}$
- Plot of residuals $\boldsymbol{e}$ against columns from the model matrix, $\mathbf{X}$
- Plot of residuals $\boldsymbol{e}$ against omitted variables


Any local pattern or patterns (e.g., quadratic trend, cycles, changepoints, subgroups) are indicative of misspecification of the mean model.

Use local smoother (GAM or LOESS) to detect trends.

## Examples of residual plots


Look for pattern in the $y$-axis, not the $x$-axis!


```{r}
#| label: fig-regdiaglin
#| echo: false
#| fig-cap: Scatterplots of residuals against fitted values. The first two plots show
#|   no departure from linearity (mean zero). The third plot shows a clear quadratic
#|   pattern, suggesting the mean model is misspecified. Note that the distribution of
#|   the fitted value need not be uniform, as in the second panel which shows more high
#|   fitted values.
par(pch = 19, col = scales::alpha("black", 0.8), bty = "l")
par(mar=c(3.1, 3.1, 1, 1), mgp=c(1.7, 0.6, 0), font.main=1, cex.main=0.8)
par(mfrow=c(1, 3))
x1 <- 1:200
x2 <- (1:200)^2
set.seed(1)
lm1 <- lm(rnorm(200)~x1)
lm2 <- lm(rnorm(200) ~ x2)
s1 <- scale(sin(x1/45)*2+rnorm(200))
lm3 <- lm(s1 ~ x1)
car::residualPlot(lm1,ylab="residuals", xlab = "fitted values", col.quad = hecblue)
car::residualPlot(lm2,ylab="residuals", xlab = "fitted values", col.quad = hecblue)
car::residualPlot(lm3,ylab="residuals", xlab = "fitted values", col.quad = hecblue)
```

## Examples 


```{r}
#| fig-cap: "Scatterplot of residuals against explanatory (left) and an omitted covariate (right). We can pick up a forgotten interaction between BMI and smoker/obese and a linear trend for the number of children."
#| label: fig-residplots
data(insurance, package = "hecstatmod")
insurance <- insurance |>
  dplyr::mutate(obesity = factor(bmi >= 30, levels = c(FALSE, TRUE), 
                                 labels = c("non-obese","obese")))
mod0 <- lm(charges ~ age + obesity*smoker, data = insurance)
mod1 <- lm(charges ~ age + obesity*smoker*bmi, data = insurance)
mod2 <- lm(charges ~ age + obesity*smoker*bmi + children, data = insurance)
g1 <- ggplot(data = data.frame(e = resid(mod0),
                         x = insurance$bmi, 
                         col = interaction(insurance$obesity, insurance$smoker)),
       mapping = aes(x = x, y = e, color = col)) +
  geom_point() +
  geom_smooth(se = FALSE,method = "lm", formula = y ~ x, show.legend = FALSE) +
  MetBrewer::scale_color_met_d(name = "Hiroshige") + 
  labs(x = "body mass index", y = "ordinary residuals",
       color = "obese/smoker") +
  theme(legend.position = "bottom")
g2 <- ggplot(data = data.frame(e = resid(mod1),
                         x = insurance$children),
       mapping = aes(x = x, y = e)) +
  geom_jitter() +
  geom_smooth(method = "lm", formula = y ~ x, col = "black",
              se = FALSE) +
  MetBrewer::scale_color_met_d(name = "Hiroshige") + 
  labs(x = "number of children", y = "ordinary residuals")
g1 + g2
```

## Examples for the college data

```{r}
#| eval: true
#| echo: true
#| out-width: '100%'
#| fig-align: 'center'
#| fig-width: 10
#| fig-height: 8
data(college, package = "hecstatmod")
linmod.college1 <- lm(salary ~ rank + field + sex + service + years, data = college)
car::residualPlots(linmod.college1, test = FALSE, layout = c(2,3))
```

## Remedy for mean model specification

Fix the mean model

- Add covariates that are important explanatories
- Include interactions if necessary
- For residual patterns, specify the effect of nonlinear terms via penalized splines
- Transformations

## Assumption 2: homoscedasticity (equal variance)

The variance is the same for all observations, $\mathsf{Va}(Y_i \mid \mathbf{x}_i) = \sigma^2$

Typical heteroscedasticity patterns arise when

- Variance varies per levels of a categorical variable
- Variance increases with the response (typically multiplicative models)
- Data are drawn from a distribution whose variance depends on the mean, e.g., Poisson

## Diagnostic for equal variance

Use externally studentized residuals $r_i$, which have equal variance.

Hypothesis tests:

- Levene test (fit ANOVA to $|r_{ij} - \overline{r_{j}}|$ as a function of group index $j \in \{1, \ldots, J\}$)
- Breusch--Pagan test (popular in economics, fits linear regression to $e_i^2$)
- Bartlett test (normal likelihood ratio test for different variance, but very sensitive to normality assumption so not recommended)

Graphical diagnostics

- Plot (absolute value of) $r_i$ against fitted values (spread-level plot)

## Examples of spread level plots


```{r}
#| label: fig-diagfitvalhomosce
#| echo: false
#| fig-cap: Plot of externally studentized residuals against fitted value (left) and categorical
#|   explanatory (right). Both clearly display heteroscedasticity.
par(mar=c(3.1,3.1,1.6,1),
    mgp=c(1.7,0.6,0),font.main=1,cex.main=0.8,
    mfrow= c(1,3),  pch = 19, bty = "l")
set.seed(1)
x1 <- sort(100+3*rnorm(200,0,10))
s1 <- scale(rnorm(200)*(x1-30)^2)
fithetero <- rstudent(lm(s1~x1))
plot(x1,s1,xlab="fitted values",ylab="externally studentized residuals")
plot(x1,abs(fithetero),xlab="fitted values",ylab="|externally studentized residuals|")
car::gamLine(x1, abs(fithetero), spread = TRUE, col = hecblue)
set.seed(1)
fitlmh <- lm(c(rnorm(100, sd = 0.5), rnorm(100, sd = 3), rnorm(30, sd = 1.5)) ~ (xcat <- factor(c(rep(1,100), rep(2, 100), rep(3, 30)))))
boxplot(rstudent(fitlmh) ~ xcat, xlab = "group", ylab="externally studentized residuals")
```

## Heteroscedasticity tests for college data

```{r}
#| eval: true
#| echo: true
# Extract externally studentized residuals
r <- rstudent(linmod.college1)
# Levene test (F-test for ANOVA)
car::leveneTest(r ~ rank, center = "mean", data = college)
# Breusch-Pagan (with a score test)
car::ncvTest(linmod.college1, var.formula =  ~ rank)
```

## Consequences of unequal variance


```{r}
#| label: fig-simuWelchnull
#| echo: false
#| cache: true
#| fig-cap: Histogram of the null distribution of $p$-values obtained through simulation
#|   using the two-sample $t$-test (left) and Welch's $t$-test (right), based on 10 000 simulations. Each simulated sample consist
#|   of 50 observations from a $\mathsf{normal}(0, 1)$ distribution and 10 observations from
#|   $\mathsf{normal}(0, 9)$. The uniform distribution would have 5% in each of the 20 bins
#|   used for the display.
set.seed(1234)
n1 <- 50
n2 <- 10
mu1 <- 0
mu2 <- 0
sd1 <- 1
sd2 <- 3
nrep <- 1e4
pvalsF <- rep(0, nrep)
pvalsW <- rep(0, nrep)
group <- factor(c(rep(0, n1), rep(1, n2)))
for(i in seq_len(nrep)){
dat <- c(rnorm(n = n1, mean = mu1, sd = sd1),
         rnorm(n = n2, mean = mu2, sd = sd2))
pvalsW[i] <- t.test(dat ~ group)$p.value
pvalsF[i] <- t.test(dat ~ group, var.equal = TRUE)$p.value
}
g1 <- ggplot(data = data.frame("pvalue" = pvalsF),
       aes(x = pvalue)) +
  # bin into 20 compartments,
  # specifying boundaries to avoid artifacts
  geom_histogram(breaks = seq(0, 1, by = 0.05),
                 aes(y = after_stat(width*density)),
                 alpha = 0.2) +
  stat_function(fun = function(x){1/20}, #uniform distribution
                col = "blue") +
  labs(x = "p-value",
       y = "percentage",
       caption = "two sample t-test (equal variance)") +
   scale_x_continuous(expand = c(0, 0),
                      limits = c(0, 1),
                      breaks = c(0,0.5,1))
g2 <- ggplot(data = data.frame("pvalue" = pvalsW),
       aes(x = pvalue)) +
  # bin into 20 compartments,
  # specifying boundaries to avoid artifacts
  geom_histogram(breaks = seq(0, 1, by = 0.05),
                 aes(y = after_stat(width*density)),
                 alpha = 0.2) +
  stat_function(fun = function(x){0.05}, #uniform distribution
                col = "blue") +
  labs(x = "p-value",
       y = "percentage",
       caption = "Welch t-test (unequal variance)") +
   scale_x_continuous(expand = c(0, 0),
                      limits = c(0, 1),
                      breaks = c(0,0.5,1))
theme_set(theme_classic())
g1 + g2
```

##  Remedy 1 - specify the variance structure

Specify a function for the variance, e.g., 

- $\sigma_j$ for level $j$ of a categorical variable, 
- $\sigma^2(\boldsymbol{v}_i) = g(\boldsymbol{v}_i; \boldsymbol{\theta})$ for some suitable transformation $g(\cdot): \mathbb{R} \to (0, \infty)$, some covariate vector $\boldsymbol{v}$ and parameter $\boldsymbol{\theta}$.

A model specification enables the use of likelihood ratio tests.


The model can be fitted via restricted maximum likelihood using the function `gls` from package `nlme`.

## Example of heteroscedasticity for the college data

For the college data, we set $Y_i \sim \mathsf{normal}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2_{\texttt{rank}_i})$ with three different variance parameters. This seemingly corrects the heteroscedasticity.


```{r}
#| eval: true
#| echo: true
#| fig-align: 'center'
library(nlme) # R package for mixed models and variance specification
linmod.college2 <- nlme::gls(
  model = salary ~ rank + field + sex + service, # mean specification
  weights = nlme::varIdent(form = ~1 | rank), # constant variance per rank
  data = college)
plot(linmod.college2)
```


## Group heteroscedasticity in ANOVA

The  `t.test` and `oneway.test` functions in **R** perform hypothesis test for equality of mean: for observation $i$ from group $j$, the model specified is

$$Y_{ij} \sim \mathsf{normal}(\mu_j, \sigma^2_j).$$
These tests are often labelled by software under the name of @Welch:1947.

There is no exact expression for the null distribution for equal variance, but approximations due to @Satterthwaite:1946 are used as benchmarks.

We need enough people in each subgroup to reliably estimate both the mean and variance!

## Remedy 2 - use a sandwich matrix for the errors

Economists often use **sandwich** estimators [@White:1980], whereby we replace the estimator of the covariance matrix of $\widehat{\boldsymbol{\beta}}$, usually $S^2(\mathbf{X}^\top\mathbf{X})^{-1}$, by a sandwich estimator of the form

$$\widehat{\mathsf{Va}}_{\mathsf{HCE}}(\boldsymbol{\widehat{\beta}}) = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\Omega}\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}$$
with $\boldsymbol{\Omega}$ a diagonal matrix.

Popular choices are heteroscedastic consistent matrices [@McKinnon.White:1985], e.g., taking $\mathrm{diag}(\boldsymbol{\Omega})_i = e_i^2/(1-h_{ii})^2$, the so-called  HC${}_3$.

## Example of sandwich matrix

Replace $\mathsf{Va}(\widehat{\boldsymbol{\beta}})$ by $\widehat{\mathsf{Va}}_{\mathsf{HCE}}(\boldsymbol{\widehat{\beta}})$ in the formula of Wald tests. 

```{r}
#| echo: true
vcov_HCE <- car::hccm(linmod.college1)
# Wald tests with sandwich matrix
w <- coef(linmod.college1) / sqrt(diag(vcov_HCE))
# Variance ratios
diag(vcov_HCE) / diag(vcov(linmod.college1))
# Compute p-values
pval <- 2*pt(abs(w), 
             df = linmod.college1$df.residual,
             lower.tail = FALSE)
```



## Multiplicative structure

Multiplicative data of the form
\begin{align*}
\left(\begin{matrix} \text{moyenne $\mu$}\end{matrix}\right) \times
 \left(\begin{matrix} \text{random error}
\end{matrix}\right)
\end{align*}
tend to have higher variability when the response is larger.

## Remedy 3 - Variance-stabilizing transformations

A log-transformation of the response, $\ln Y$, makes the model **additive**, assuming $Y > 0$.

Write the log-linear model
\begin{align*}
\ln Y = \beta_0+ \beta_0 +\beta_1 X_1 +\cdots + \beta_pX_p + \varepsilon
\end{align*}
in the original response scale as
\begin{align*}
Y &= \exp\left(\beta_0 +\beta_1 X_1 +\cdots + \beta_pX_p\right)\cdot \exp(\varepsilon),
\end{align*}
and thus
\begin{align*}
\mathsf{E}(Y \mid \mathbf{X}) = \exp(\beta_0 +\beta_1 X_1 +\cdots + \beta_pX_p) \times \mathsf{E}\{\exp(\varepsilon) \mid \mathbf{X}\}.
\end{align*}

## Lognormal model

If $\varepsilon \mid \mathbf{x} \sim \mathsf{normal}(\mu,\sigma^2)$, then $\mathsf{E}\{\exp(\varepsilon) \mid \mathbf{x}\}= \exp(\mu+\sigma^2/2)$ and $\exp(\varepsilon)$ follows a log-normal distribution.

An increase of one unit of $X_j$ leads to a $\beta_j$ increase of $\ln Y$ without interaction or nonlinear term for $X_j$, and this translates into a multiplicative increase of a factor $\exp(\beta_j)$ on the original data scale for $Y$. 

- If $\beta_j=0$, $\exp(\beta_j)=1$ and there is no change
- If $\beta_j < 0$, $\exp(\beta_j)<1$ and the mean decreases with $X_j$
- If $\beta_j > 0$, $\exp(\beta_j)>1$ and the mean increases with $X_j$


## Interpretation of log linear models

Compare the ratio of $\mathsf{E}(Y \mid X_1=x+1)$ to $\mathsf{E}(Y \mid X_1=x)$,
\begin{align*}
\frac{\mathsf{E}(Y \mid X_1=x+1, X_2, \ldots, X_p)}{\mathsf{E}(Y \mid X_1=x,  X_2, \ldots, X_p)} = \frac{\exp\{\beta_1(x+1)\}}{\exp(\beta_1 x)} = \exp(\beta_1).
\end{align*}
Thus, $\exp(\beta_1)$ represents the ratio of the mean of $Y$ when $X_1=x+1$ in comparison to that when $X_1=x$, *ceteris paribus* (and provided this statement is meaningful).



The percentage change is 

- $1-\exp(\beta_j)$ if $\beta_j <0$ and 
- $\exp(\beta_j)-1$ if $\beta_j>0$.

## More general transformations

Consider the case where both $Y$ and $X_1$ is log-transformed, so that
\begin{align*}
Y= X_1^{\beta_1}\exp(\beta_0 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon)
\end{align*}
Taking the derivative of the left hand side with respect to $X_1>0$, we get
\begin{align*}
\frac{\partial Y}{\partial X_1}&= \beta_1 X_1^{\beta_1-1}\exp(\beta_0 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon)= \frac{\beta_1 Y}{X_1}
\end{align*}
and thus we can rearrange the expression so that
\begin{align*}
\frac{\partial X_1}{X_1}\beta_1 = \frac{\partial Y}{Y};
\end{align*}
this is a partial **elasticity**, so $\beta_1$ is interpreted as a $\beta_1$ percentage change in $Y$ for each percentage increase of $X_1$, *ceteris paribus*.

## Independence assumption

Follows from sampling scheme (*random sample*), need context to infer whether this assumption holds.

Typical violations include

- repeated measures (correlated observations).
- longitudinal data: repeated measurements are taken from the same subjects (few time points)
- time series: observations observed at multiple time periods
- spatial data

## Consequences of dependence

Nearby things are more alike, so the amount of 'new information' is smaller than the sample size.


When observations are positively correlated, the estimated standard errors reported by the software are too small. 


This means we are overconfident and will reject the null hypothesis more often then we should if the null is true (inflated Type I error, or false positive).

## Consequences of correlated data

```{r }
#| label: fig-plotLevelIndep
#| echo: false
#| eval: true
#| fig-cap: Percentage of rejection of the null hypothesis for the $F$-test of equality
#|   of means for the one way ANOVA with data generated with equal mean and variance
#|   from an equicorrelation model (within group observations are correlated, between
#|   group observations are independent). The nominal level of the test is 5%.
# size_p5 <- t(apply(matRes, 2:3, function(x){mean(x < 0.05)}))
# nent <- length(size_p5)
# size_p5_tibble <-
#   tibble(size = c(size_p5),
#          samp = factor(rep(ng, length.out = nent)),
#          correlation = rep(correlation, each = length(ng)))


# From Don Fraser (1958). Statistics: an introduction, pp. 342-343
nrep <- 25L
rho <- seq(0, 0.8, by = 0.01)
dims <- c(2,3,5,10)
size <- matrix(0, nrow = length(rho), ncol = length(dims))
for(i in seq_along(dims)){
  d <- dims[i]
  sigmasq <- (1-rho)
  tausq <- rho
  fact <- (nrep*tausq + sigmasq)/sigmasq

  cutoff <- qf(0.95, df1 = d-1, df2 = (nrep-1)*d)
  size[,i] <- pf(cutoff/fact, df1 = d-1, df2 = (nrep-1)*d, lower.tail = FALSE)
}

size_exact <- data.frame(
  size = c(size),
  dim = factor(rep(dims, each = length(rho))),
  rho = rep(rho, length.out = length(size)))

# ggplot(data = size_p5_tibble,
#        mapping = aes(col = samp,
#                      x = correlation,
#                      y = size)) +
#   geom_line() +
ggplot() +
  geom_line(data = size_exact,
            mapping = aes(color = dim, x = rho, y = size)) +
  geom_hline(yintercept = 0.05, alpha = 0.5) +
  labs(y = "size of test",
       x = "within group correlation",
       color = "number of groups",
       caption = "25 observations per group") +
  coord_cartesian(xlim = c(0, max(rho)),
                  ylim = c(0, 1),
                  expand = FALSE) +
  theme_minimal() +
  theme(legend.position = "bottom")


```

## Fixes for group structure and autocorrelation

Chapter 6 will deal with **correlated data**

The main idea is to assume instead that $$\boldsymbol{Y} \mid \mathbf{X} \sim \mathsf{normal}_n(\mathbf{X}\boldsymbol{\beta}, \boldsymbol{\Sigma})$$ and model explicitly the $n \times n$ variance matrix $\boldsymbol{\Sigma}$, parametrized in terms of covariance parameters $\boldsymbol{\psi}$.

## Time series and longitudinal data

For time series, we can look instead at a correlogram, i.e., a bar plot of the correlation between two observations $h$ units apart as a function of the lag $h$.

For $y_1, \ldots, y_n$ and constant time lags $h=0, 1, \ldots$ units, the autocorrelation at lag $h$ is [@Brockwell.Davis:2016, Definition 1.4.4]
\begin{align*}
r(h) = \frac{\gamma(h)}{\gamma(0)}, \qquad \gamma(h) = \frac{1}{n}\sum_{i=1}^{n-|h|} (y_i-\overline{y})(y_{i+h} - \overline{y})
\end{align*}

## Example of correlogram


```{r}
#| label: fig-correlogram
#| echo: false
#| fig-cap: Correlogram of independent observations (left) and the ordinary residuals
#|   of the log-linear model fitted to the air passengers data (right). While the mean
#|   model of the latter is seemingly correctly specified, there is residual dependence
#|   between monthly observations and yearly (at lag 12). The blue lines give approximate
#|   pointwise 95\% confidence intervals for white noise (uncorrelated observations).
library(forecast)
data(airpassengers, package = "hecstatmod")
ols_airpass <- lm(log(passengers) ~ month + year, data = airpassengers)
set.seed(1234)
g0 <- ggAcf(rnorm(100),lag.max=25) + labs(x = "lag", y ="autocorrelation", title = "")+ ylim(c(-1,1))
g1 <- ggAcf(resid(ols_airpass),lag.max=25) + labs(x = "lag", y ="autocorrelation", title = "") + ylim(c(-1,1))
g0 + g1
```


## Normality assumption

Without doubt the least important assumption.

Ordinary least squares are **best linear unbiased estimators** (BLUE) if the data are independent and the variance is constant, regardless of normality.

They are still unbiased and consistent if the variance is misspecified.

Tests for **parameters** are valid provided that each coefficient estimator is based on a **sufficient number of observations**.

- watch out for interactions with categorical variables (small subgroups).


## Quantile-quantile plots

Produce Student quantile-quantile plots of externally studentized residuals $R_i \sim \mathsf{Student}(n-p-2)$.

```{r}
#| label: fig-qqplotresid
#| cache: true
#| echo: false
#| fig-cap: Histogram (left) and Student quantile-quantile plot (right) of the externally
#|   studentized residuals. The left panel includes a kernel density estimate (black),
#|   with the density of Student distribution (blue) superimposed. The right panel includes
#|   pointwise 95\% confidence bands calculated using a bootstrap.
par(pch = 19, col = scales::alpha("black", 0.8), bty = "l")
par(mar=c(3.1, 3.1, 1, 1), mgp=c(1.7, 0.6, 0), font.main=1, cex.main=0.8)
par(mfrow = c(1,2))
library(car)
library(qqplotr, warn.conflicts = FALSE)
set.seed(1234)
di <- "t"
dp <- list(df = lm2$df.residual)
de <- FALSE
g2 <- ggplot(data = data.frame(sample = rstudent(lm2)), mapping = aes(sample = sample)) +
 stat_qq_band(distribution = di, detrend = de, bandType = "boot", B = 9999, dparams = dp) +
 stat_qq_line(distribution = di, detrend = de) +
 stat_qq_point(distribution = di, detrend = de) +
 labs(x = "theoretical Student quantiles", y = "externally studentized residuals")
g1 <- ggplot(data = data.frame(x = rstudent(lm2)), aes(x=x)) +
  geom_histogram(aes(x=x, y = after_stat(density))) +
  stat_function(fun = "dt", args = list(df = lm2$df.residual), col = hecblue) +
  geom_density() + labs(x = "externally studentized residuals")
g1 + g2
```

## Interpretation of quantile-quantile plots



```{r}
#| label: fig-qqplotsbad
#| cache: true
#| echo: false
#| fig-cap: Quantile-quantile plots of non-normal data, showing typical look of behaviour
#|   of discrete (top left), heavy tailed  (top right), skewed (bottom left) and bimodal
#|   data (bottom right).
set.seed(1234)
par(pch = 19, col = scales::alpha("black", 0.8), bty = "l")
par(mar=c(3.1, 3.1, 1, 1), mgp=c(1.7, 0.6, 0), font.main=1, cex.main=0.8)
par(mfrow= c(2,2))
qqPlot(scale(rgeom(n = 100, prob = 0.5)),
       xlab = "theoretical quantiles",
       ylab = "empirical quantiles",
       col.lines=hecblue, id = FALSE)
set.seed(123)
qqPlot(scale(rt(200, df = 3)),
       xlab = "theoretical quantiles",
       ylab = "empirical quantiles",
       col.lines=hecblue, id = FALSE)
set.seed(432)
qqPlot(scale(rgamma(100,shape = 2)),
       xlab = "theoretical quantiles",
       ylab = "empirical quantiles",
       col.lines=hecblue, id = FALSE)
set.seed(432)
qqPlot(scale(c(rnorm(100,-3),rnorm(100,3))),
       xlab = "theoretical quantiles",
       ylab = "empirical quantiles",
       col.lines=hecblue, id = FALSE)
```

## Remedy for normality

- If data arise from different families (Poisson or negative binomial counts, binomial data for proportions and binary, etc.), use **generalized linear models**.
- Box--Cox type transformations

## Box--Cox transformation


For strictly positive data, one can consider a Box--Cox transformation,
\begin{align*}
y(\lambda)= \begin{cases}
(y^{\lambda}-1)/\lambda, & \lambda \neq 0\\
\ln(y), & \lambda=0.
\end{cases}
\end{align*}
The cases 

- $\lambda=-1$ (inverse), 
- $\lambda=1$ (identity) and 
- $\lambda=0$ (log-linear model) 

are perhaps the most important because they yield interpretable models.

## Inference for Box--Cox models

If we assume that $\boldsymbol{Y}(\lambda) \sim \mathsf{normal}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}_n)$, then the likelihood is
\begin{align*}
L(\lambda, \boldsymbol{\beta}, \sigma; \boldsymbol{y}, \mathbf{X}) &= (2\pi\sigma^2)^{-n/2} J(\lambda, \boldsymbol{y}) \times\\& \quad \exp \left[ - \frac{1}{2\sigma^2}\{\boldsymbol{y}(\lambda) - \mathbf{X}\boldsymbol{\beta}\}^\top\{\boldsymbol{y}(\lambda) - \mathbf{X}\boldsymbol{\beta}\}\right],
\end{align*}
where $J$ denotes the Jacobian of the Box--Cox transformation, $J(\lambda, \boldsymbol{y})=\prod_{i=1}^n y_i^{\lambda-1}$.

## Profiling $\lambda$

For each given value of $\lambda$, the maximum likelihood estimator is that of the usual regression model, with $\boldsymbol{y}$ replaced by $\boldsymbol{y}(\lambda)$.

The profile log likelihood for $\lambda$ is
\begin{align*}
\ell_{\mathsf{p}}(\lambda) = -\frac{n}{2}\ln(2\pi \widehat{\sigma}^2_\lambda) - \frac{n}{2} + (\lambda - 1)\sum_{i=1}^n \ln(y_i)
\end{align*}


## Box--Cox transform for the poison data

@Box.Cox:1964 considered survival time for 48 animals based on a randomized trial. The `poisons` data are balanced, with 3 poisons were administered with 4 treatments to four animals each.

We could consider a two-way ANOVA without interaction, given the few observations for each combination. The model would be of the form
\begin{align*}
Y &= \beta_0 + \beta_1 \texttt{poison}_2 + \beta_2\texttt{poison}_3  +\beta_3\texttt{treatment}_2 \\ &\qquad+ \beta_4\texttt{treatment}_3
+\beta_5\texttt{treatment}_4 + \varepsilon
\end{align*}


## Diagnostic plots for poison data

```{r}
#| label: fig-poisonplots
#| echo: false
#| eval: true
#| fig-cap: "Diagnostic plots for the poison data: ordinary residuals (jittered)
#|   for the linear model for survival time as a function of poison and treatment  and
#|   fitted values against residuals."
#| fig-align: 'center'
poisons <- SMPracticals::poisons
poisonlm1 <- lm(time ~ poison + treat, data = poisons)
poisonlm2 <- lm(I(1/time) ~ poison + treat, data = poisons)

poisons$resid1 <- resid(poisonlm1)
poisons$rstudent1 <- rstudent(poisonlm1)
poisons$resid2 <- resid(poisonlm2)
poisons$rstudent2 <- rstudent(poisonlm2)
poisons$fitted1 <- fitted(poisonlm1)
poisons$fitted2 <- fitted(poisonlm2)
g1 <- ggplot(data = poisons) +
  geom_point(aes(x = fitted1, y = resid1), position = position_jitter(width = 0.05)) +
  labs(y = "ordinary residuals", x = "fitted values")
dp <- list(df=poisonlm1$df.residual-1)
di <- "t"
de <- FALSE
library(qqplotr)
g2 <- ggplot(data = poisons, aes(sample = rstudent1)) +
 stat_qq_band(distribution = di, dparams = dp,
              detrend = de, identity = TRUE,
              bandType = "boot", B = 9999) +
 stat_qq_line(distribution = di, dparams = dp,
              detrend = de, identity = TRUE) +
 stat_qq_point(distribution = di, dparams = dp,
               detrend = de, identity = TRUE) +
 labs(x = "theoretical quantiles",
      y = "empirical quantiles")
g4 <- ggplot(data = poisons, aes(sample = rstudent2)) +
 stat_qq_band(distribution = di, dparams = dp,
              detrend = de, identity = TRUE,
              bandType = "boot", B = 9999) +
 stat_qq_line(distribution = di, dparams = dp,
              detrend = de, identity = TRUE) +
 stat_qq_point(distribution = di, dparams = dp,
               detrend = de, identity = TRUE) +
 labs(x = "theoretical quantiles",
      y = "empirical quantiles")
g3 <- ggplot(data = poisons) +
  geom_point(aes(x = fitted2, y = resid2), position = position_jitter(width = 0.05)) +
  labs(y = "ordinary residuals", x = "fitted values")
boxcox_gg <- function(fitted.lm, showlambda = TRUE, lambdaSF = 3, grid = seq(-2,2, by = 0.1), scale.factor = 0.5) {
      boxcox_object <- MASS::boxcox(fitted.lm, lambda = grid, plotit = FALSE)
    x <- unlist(boxcox_object$x)
    y <- unlist(boxcox_object$y)
    xstart <- x[-1]
    ystart <- y[-1]
    xend <- x[-(length(x))]
    yend <- y[-(length(y))]
    boxcox_unlist <- data.frame(xstart, ystart, xend, yend)
    best_lambda <- x[which.max(y)]
    rounded_lambda <- round(best_lambda, lambdaSF)
    min_y <- min(y)
    accept_inds <- which(y > max(y) - 1/2 * qchisq(0.95, 1))
    accept_range <- x[accept_inds]
    conf_lo <- round(min(accept_range), lambdaSF)
    conf_hi <- round(max(accept_range), lambdaSF)
    plot <- ggplot(data = boxcox_unlist) + geom_segment(aes(x = xstart,
        y = ystart, xend = xend, yend = yend), linewidth = scale.factor) +
        labs(x = expression(lambda), y = "profile log likelihood") +
        geom_vline(xintercept = best_lambda, linetype = "dotted",
            linewidth = scale.factor/2) + geom_vline(xintercept = conf_lo,
        linetype = "dotted", linewidth = scale.factor/2) + geom_vline(xintercept = conf_hi,
        linetype = "dotted", linewidth = scale.factor/2) + geom_hline(yintercept = y[min(accept_inds)],
        linetype = "dotted", linewidth = scale.factor/2)
    if (showlambda) {
        return(plot +
                 annotate("text", x = best_lambda, label = as.character(rounded_lambda), y = min_y) +
              annotate("text", x = conf_lo, label = as.character(conf_lo), y = min_y) +
              annotate("text", x = conf_hi,
            label = as.character(conf_hi), y = min_y))
    } else {
        return(plot)
    }
}
g5 <- boxcox_gg(poisonlm1, grid = seq(-1.5,0.1, by = 0.01))
g1 + g2
```

## Profile plot

```{r}
#| fig-align: 'center'
g5
```

The profile log likelihood for the Box--Cox transform parameter, suggests a value of $\lambda=-1$ would be within the 95\% confidence interval. 

## Model with transformation

The reciprocal response $Y^{-1}$ corresponds to the speed of action of the poison depending on both poison type and treatment. 

```{r}
#| fig-align: 'center'
g3 + g4
```

The diagnostics plot for this model show no residual structure.


## Comment about transformations

We cannot compare models fitted to $Y_i$ versus $\ln Y_i$ using, e.g., information criteria or test, because models have different responses.

We can use however the Box--Cox likelihood, which includes the **Jacobian** of the transformation, to assess the goodness of fit and compare the model with $\lambda=0$ versus $\lambda=-1$.

## Diagnostics for outliers

Outliers can impact the fit, the more so if they have high leverage.

Plot the Cook distance $C_i$ as a function of the leverage $h_{ii}$, where 
\begin{align*}
C_i=\frac{r_i^2h_{ii}}{(p+1)(1-h_{ii})}.
\end{align*}

We can also test for the residuals to flag abnormal values using `car::outlierTest`.

## Diagnostic plots for the insurance data


```{r}
#| eval: true
#| echo: false
#| fig-cap: "Diagnostic plots for outliers."
#| label: fig-cook
library(ggfortify)
autoplot(mod2, which = 5:6)
car::outlierTest(mod2)
```


## Remedies for outliers

- Remove them (not recommended)
- Use robust regression, which automatically downweights observations

```{r}
#| echo: true
rmod_ins <- MASS::rlm(data = insurance,
  charges ~ splines::bs(age) + obesity*smoker*bmi + children)
```

Robust regression is less efficient (higher std. errors), but more robust to outliers.

The theory of robust statistics beyond the scope of the course.

## References

