---
title: "Statistical modelling"
author: "Léo Belzile, HEC Montréal"
subtitle: "06. Linear models (collinearity)"
date: today
date-format: YYYY
eval: true
cache: false
echo: false
fig-align: 'center'
out-width: '100%'
standalone: true
bibliography: MATH60604A.bib
format:
  revealjs:
    slide-number: true
    preview-links: auto
    code-block-height: 750px
    theme: [simple, hecmontreal.scss]
    title-slide-attributes:
      data-background-color: "#002855"
    logo: "fig/logo_hec_montreal_bleu_web.png"
    width: 1600
    height: 900
---


```{r}
#| eval: true
#| include: false
#| cache: false
hecblue <- c("#002855")

fcols <- c(gris = "#888b8d",
           bleu = "#0072ce",
           aqua = "#00aec7",
           vert = "#26d07c",
           rouge = "#ff585d",
           rose = "#eb6fbd",
           jaune = "#f3d03e")
pcols <- c(gris = "#d9d9d6",
           bleu = "#92c1e9",
           agua = "#88dbdf",
           vert = "#8fe2b0",
           rouge = "#ffb1bb",
           rose = "#eab8e4",
           jaune = "#f2f0a1")
library(ggplot2)
library(knitr)
theme_set(theme_classic())
library(patchwork)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(scipen=1, digits=3, width = 75)
# options(knitr.kable.NA = '')
# library(kableExtra)
library(car)
```


## Multicollinearity

- Multicollinearity describes when an explanatory variable (or several) is strongly correlated with a linear combination of other explanatory variables.
- One potential harm of multicollinearity is the *decrease in precision*: it increases the standard errors of the parameters. 


## Bixi and multicollinearity


We consider a simple illustration with temperature at 16:00 in Celcius and Farenheit (rounded to the nearest unit for $\texttt{rfarenheit}$) to explain log of daily counts of Bixi users for 2014--2019. 
 
```{r}
data(bixicoll, package = "hecstatmod")
knitr::kable(head(bixicoll, n = 5L), booktabs = TRUE, digits = c(2,2,2,1))
```

## Linear invariance

Consider the log number of Bixi rentals per day as a function of the temperature in degrees Celcius and in Farenheit, rounded to the nearest unit. The postulated linear model is 
\begin{align*}
\texttt{lognuser} = \beta_0 + \beta_{\texttt{c}} \texttt{celcius} + \beta_{\texttt{f}} \texttt{farenheit} + \varepsilon.
\end{align*}

- The interpretation of $\beta_{\texttt{c}}$ is "the average increase in number of rental per day when temperature increases by $1{}^{\circ}$C, keeping the temperature in Farenheit constant"...
- The two temperatures units are linearly related,
$$1.8 \texttt{celcius} + 32 = \texttt{farenheit}.$$

## Diving into the problem

Suppose that the true effect (fictional) effect of temperature on bike rental is 
 \begin{align*}
  \mathsf{E}(\texttt{lognuser} \mid \cdot) = \alpha_0+ \alpha_1 \texttt{celcius}.
 \end{align*}
 
The coefficients for the model that only includes Farenheit are thus
 \begin{align*}
  \mathsf{E}(\texttt{lognuser} \mid \cdot)= \gamma_0 + \gamma_1\texttt{farenheit},
 \end{align*}
 where $\alpha_0 = \gamma_0 + 32\gamma_1$ and $1.8\gamma_1 = \alpha_1$.
 
```{r}
#| eval: true
#| echo: false
#| layout-ncol: 2
data(bixicoll, package = "hecstatmod")
linmod1_bixicoll <- lm(lognuser ~ celcius, data = bixicoll)
linmod2_bixicoll  <- lm(lognuser ~ farenheit, data = bixicoll)
knitr::kable(summary(linmod1_bixicoll)$coefficients[,1:2], 
             booktabs = TRUE,
             digits = 3)
knitr::kable(summary(linmod2_bixicoll)$coefficients[,1:2], 
             booktabs = TRUE,
             digits = 3)
```


## Perfect collinearity

The parameters of the postulated linear model with both predictors, 
 \begin{align*}
 \texttt{lognuser} = \beta_0 + \beta_{\texttt{c}} \texttt{celcius} + \beta_{\texttt{f}} \texttt{farenheit} + \varepsilon,
\end{align*}
 are not **identifiable**, since any linear combination of the two solutions 
gives the same answer.


This is the same reason why we include $K-1$ dummy variables for a categorical variable with $K$ levels when the model already includes an intercept.


## Lack of uniqueness of the solution

```{r}
#| eval: true
#| echo: true
# Exact collinearity
linmod3_bixicoll  <- lm(lognuser ~ celcius + farenheit, data = bixicoll)
summary(linmod3_bixicoll) 
```

## Estimated coefficients with near-collinearity

```{r}
#| eval: true
#| echo: true
# Approximate colinearity
linmod4_bixicoll  <- lm(lognuser ~ celcius + rfarenheit, data = bixicoll)
summary(linmod4_bixicoll)
```
 
## Effects of collinearity

Generally, 

- The regression coefficients change drastically when new observations are included, or when we include/remove new covariates.
- The standard errors of the coefficients in the multiple regression model are very high, since the $\boldsymbol{\beta}$ cannot be precisely estimated.
- The individual parameters are not statistically significant, but the global $F$-test indicates some covariates are nevertheless relevant.


## Detecting collinearity

If the variables are exactly collinear, **R** will drop redundant ones.

- The variables that are not *perfectly* collinear (e.g., due to rounding) will not be captured by software and will cause issues.

Otherwise, we can look at the correlation coefficients, or better the **variance inflation factor**

## Variance inflation factor

For a given explanatory variable $X_j$, define
\begin{align*}
\mathsf{VIF}(j)=\frac{1}{1-R^2(j)}
\end{align*}
where $R^2(j)$ is the $R^2$ of the model obtained by regressing ${X}_j$ on all the other explanatory variables.

$R^2(j)$ represents the proportion of the variance of $X_j$ that is explained by all the other predictor variables.

## When is collinearity an issue?

There is no general agreement, but practitioners typically choose an arbitrary cutoff (rule of thumb) among the following


- $\mathsf{VIF}(j) > 4$ implies that $R^2(j) >0.75$
- $\mathsf{VIF}(j) > 5$ implies that $R^2(j) >0.8$
- $\mathsf{VIF}(j) > 10$ implies that $R^2(j) >0.9$

```{r}
#| eval: true
#| echo: true
car::vif(linmod4_bixicoll)
```



 
## Observations for Bixi multicollinearity example
 
- The value of the $F$ statistic for the global significance for the simple linear model with Celcius (not reported) is $1292$ with associated $p$-value less than $0.0001$, suggesting that temperature is statistically significant ($5$\% increase in number of users for each increase of $1^{\circ}$C).
 - Yet, when we include both Celcius and Farenheit (rounded), the individual coefficients are not significant anymore at the 5\% level.
 - Moreover, the sign of `rfarenheit` change relative to that of `farenheit`!
  - Note that the standard errors for Celcius are $48$ times bigger when including the two covariates.
 - The variance inflation factors of both `rfarenheit` and `celcius` are enormous, suggesting identifiability issues. 

## Added variable plots

We can also use graphics to check suspicious relationships.

- Remove the column of the model matrix $\mathbf{X}$ corresponding to explanatory variable $X_j$ to obtain $\mathbf{X}_{-j}$
  - fit a regression of $\boldsymbol{y}$ as a function of $\mathbf{X}_{-j}$
  - fit a linear regression of $\boldsymbol{X}_j$ as a function of $\mathbf{X}_{-j}$
  - plot both residuals. The regression slope is exactly $\beta_j$.



## Added variable plots for Bixi multicollinearity data
 
```{r}
#| eval: true
#| echo: true
#| out-width: '100%'
#| fig-align: 'center'
#| fig-width: 10
#| fig-height: 8
#| fig-cap: "Added variable plots for Bixi collinearity data. Both are collinear and show no relationship once either is included."
#| label: fig-avplot-bixi
car::avPlots(linmod4_bixicoll, id = FALSE)
```

## Example - added variable plots

```{r}
#| eval: true
#| echo: true
#| out-width: '100%'
#| fig-align: 'center'
#| fig-width: 10
#| fig-height: 8
#| fig-cap: "Added variable plots for years of service and years since PhD. Both are collinear and show no relationship once either is included."
#| label: fig-addedvariableplots
data(college, package = "hecstatmod")
linmod1_college <- lm(
  salary ~ rank + field + sex + service + years,
  data = college)
car::avPlots(linmod1_college, terms = ~service + years, id = FALSE)
```


## Confounding variable

A **confounder** is a variable $C$ that is associated with both the response $Y$ and an explanatory variable $X$ of interest.

```{mermaid}
flowchart TD
    A("explanatory") --> B("response")
    C("confounder") --> A & B
    style A color:#FFFFFF, fill:#AA00FF, stroke:#AA00FF
    style B color:#FFFFFF, stroke:#00C853, fill:#00C853
    style C color:#FFFFFF, stroke:#2962FF, fill:#2962FF
```

The confounding variable $C$ can bias the observed relationship between $X$ and $Y$, thus complicating the interpretations and conclusions of our analyses.

## Example of confounder

The academic `rank` of professors is correlated with `sex`, because there are fewer women who are full professors and the latter are on average better paid. The variable `rank` is a confounder for the effect of `sex`.

```{r}
#| eval: true
#| echo: false
#| label: tbl-out
#| tbl-cap: "Coefficients for the college salary data, for models with sex and without/with rank."
data(college, package = "hecstatmod")
linmod0_college <- lm(salary ~ sex, data = college)
linmod1_college <- lm(salary ~ sex + rank, data = college)
tab0 <- as.data.frame(summary(linmod0_college)$coefficients[,1:4])
tab1 <- as.data.frame(summary(linmod1_college)$coefficients[,1:4])
rownames(tab0) <- c("intercept","sex [woman]")
rownames(tab1) <- c("intercept","sex [woman]", "rank [associate]", "rank [full]")
tab0[,4] <- papaja::apa_p(tab0[,4])
tab1[,4] <- papaja::apa_p(tab1[,4])
knitr::kable(tab0, 
             digits = 2, 
             booktabs = TRUE,
             align = "rrrr", 
             col.names = c("coef.","std. error","stat","p value"))
knitr::kable(tab1, 
             digits = 2, 
             align = "rrrr",
             booktabs = TRUE, 
             col.names = c("coef.","std. error","stat","p value"))
```


## Stratification and regression adjustment.

How to handle confounding variables? One way of discovering and accounting for a possible confounder is through **stratification**

- Compare salary separately for each rank (each rank consists of a stratum).

Or fit both variables in a regression model.

- We'll be measuring the effect of `sex`, adjusting for the other explanatory variables, which are possible confounders. 


## Experimental vs observational data

Confounders are really only an issue in the context of observational studies. 


In experiments, randomization ensures balance across all confounders that could affect $Y$. 


In this case, we can thus make causal interpretations of the effect of $X$ on $Y$ without having to adjust for possible confounders.
