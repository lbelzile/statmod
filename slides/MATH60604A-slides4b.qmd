---
title: "Statistical modelling"
author: "Léo Belzile, HEC Montréal"
subtitle: "04. Linear models"
date: today
date-format: YYYY
eval: true
cache: true
echo: true
standalone: true
bibliography: MATH60604A.bib
format:
  revealjs:
    slide-number: true
    preview-links: auto
    code-block-height: 750px
    theme: [simple, hecmontreal.scss]
    title-slide-attributes:
      data-background-color: "#002855"
    logo: "fig/logo_hec_montreal_bleu_web.png"
    width: 1600
    height: 900
---


```{r}
#| eval: true
#| include: false
#| cache: false
hecbleu <- c("#002855")
fcols <- c(gris = "#888b8d",
           bleu = "#0072ce",
           aqua = "#00aec7",
           vert = "#26d07c",
           rouge = "#ff585d",
           rose = "#eb6fbd",
           jaune = "#f3d03e")
pcols <- c(gris = "#d9d9d6",
           bleu = "#92c1e9",
           agua = "#88dbdf",
           vert = "#8fe2b0",
           rouge = "#ffb1bb",
           rose = "#eab8e4",
           jaune = "#f2f0a1")
library(ggplot2)
theme_set(theme_classic())
library(patchwork)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(digits = 3, width = 75)
```

## Recap of the previous episode

Assuming $Y_i \sim \mathsf{normal}(\mathbf{x}_i \boldsymbol{\beta}, \sigma^2)$ for $i=1, \ldots, n$ are independent, we showed that the ordinary least squares (OLS) estimator
\begin{align*}
\widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^\top \mathbf{X}\right)^{-1}\mathbf{X}^\top \boldsymbol{y} \sim \mathsf{normal}\left\{\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\top\mathbf{X})^{-1}\right\}.
\end{align*}


- We define the $i$th ordinary residual as $e_i = y_i - \mathbf{x}_i \widehat{\boldsymbol{\beta}}$.
- The **sum of squared errors** is $\sum_{i=1}^n e_i^2 = \mathsf{SS}_e$.
- We can show that $S^2=\mathsf{SS}_e/(n-p-1)$ is an unbiased estimator of the variance $\sigma^2$.
- More importantly, $\mathsf{SS}_e \sim \sigma^2 \chi^2_{n-p-1}$ and $\mathsf{SS}_e$ is independent of $\widehat{\boldsymbol{\beta}}$.

## Prediction

If we want to predict the value of a new observation, say $Y^*$, with known explanatories $\mathbf{x}^* = (1, x^*_1, \ldots, x^*_p)$, the prediction of the value will  be $\widehat{y}^* = \mathbf{x}^*\widehat{\boldsymbol{\beta}}$ because
\begin{align*}
\mathsf{E}(\widehat{Y}^* \mid \mathbf{X}, \mathbf{x}^*) = \mathsf{E}(\mathbf{x}^*\widehat{\boldsymbol{\beta}}\mid \mathbf{X}, \mathbf{x}^*) = \mathbf{x}^*\boldsymbol{\beta}.
\end{align*}


## Prediction uncertainty

Individual observations vary more than averages: assuming the new observation is independent of those used to estimate the coefficients,
\begin{align*}
\mathsf{Va}(\widehat{Y}^* - Y^* \mid \mathbf{X}, \mathbf{x}^*) &= \mathsf{Va}(\mathbf{x}^*\widehat{\boldsymbol{\beta}} - Y^* \mid \mathbf{X}, \mathbf{x}^*)
\\&=\mathsf{Va}(\mathbf{x}^*\widehat{\boldsymbol{\beta}} \mid \mathbf{X}, \mathbf{x}^*) +\mathsf{Va}(Y^* \mid \mathbf{X}, \mathbf{x}^*)
\\& = \sigma^2\mathbf{x}^{*\vphantom{\top}}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}^{*\top} + \sigma^2,
\end{align*}


The variability of new predictions is the sum of

- the uncertainty due to the estimators (based on random data) and
- the intrinsic variance of the new observation.

## Distribution of predictions

The distribution of $Y^*$ is $Y^* \mid \mathbf{x}^* \sim \mathsf{normal}(\mathbf{x}^*\boldsymbol{\beta}, \sigma^2)$.

Using properties of the estimators, we can base the prediction interval on the Student distribution, as
\begin{align*}
\frac{Y^*-\mathbf{x}^*\widehat{\boldsymbol{\beta}}}{\sqrt{S^2\{1+\mathbf{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}^{*\top}\}}}\sim \mathsf{Student}(n-p-1).
\end{align*}
where $S^2=\mathsf{SS}_e/(n-p-1)$ is the unbiased estimator of the variance $\sigma^2$.


We obtain $1-\alpha$ **prediction interval** for $Y^*$ by inverting the test statistic,
\begin{align*}
\mathbf{x}^*\widehat{\boldsymbol{\beta}}\pm \mathfrak{t}_{n-p-1}(\alpha/2)\sqrt{S^2\{1+\mathbf{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}^{*\top}\}}.
\end{align*}

## Inference for the mean

Given a $(p+1)$ row vector of explanatories $\mathbf{x}$, we can compute a summary $\mu(\mathbf{x})=\mathbf{x}\boldsymbol{\beta}$.



Similar calculations yield the formula for pointwise **confidence intervals for the mean**,
\begin{align*}
\mathbf{x}^*\widehat{\boldsymbol{\beta}}\pm \mathfrak{t}_{n-p-1}(\alpha/2)\sqrt{S^2\mathbf{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}^{*\top}}.
\end{align*}
The two differ only because of the additional variance of individual observations.


## Example

@Sokolova:2023 consider consumer bias when assessing how eco-friendly packages are. They conjecture that, paradoxically, consumers tend to view the packaging as being more eco-friendly when the amount of cardboard or paper surrounding the box is larger, relative to the sole plastic package (e.g., cereal boxes). In Study 2A, they measures

- the perceived environmental friendliness (PEF, variable `pef`) as a function of
- the `proportion` of paper wrapping (either none, half of the area of the plastic, equal or twice).


 We fit a simple linear regression of the form $$\texttt{pef} = \beta_0 + \beta_1 \texttt{proportion} + \varepsilon$$ with $\varepsilon \sim \mathsf{normal}(0,\sigma^2)$ and observations are assumed independent.

## Prediction for simple linear regression

```{r}
#| label: fig-predinterval
#| cache: true
#| echo: false
#| message: false
#| warning: false
#| fig-cap: Mean predictions with prediction intervals (left) and confidence intervals for the mean (right).
#| fig-format: png
library(gganimate)
data(SKD23_S2A, package = "hecedsm") # load data
lm_simple <- lm(pef ~ proportion, data = SKD23_S2A) # fit simple linear regression
# <- lm(intention ~ fixation, data = intention)
predci <- data.frame(cbind(proportion = seq(0, 2, by = 0.1),
                predict(lm_simple, newdata = data.frame(proportion = seq(0, 2, by = 0.1)),
                        interval = "c")[,-1]))
predpi <- data.frame(cbind(proportion = seq(0, 2, by = 0.1),
                predict(lm_simple, newdata = data.frame(proportion = seq(0, 2, by = 0.1)),
                        interval = "p")[,-1]))
nsim <- 25L
boot_dat <- data.frame(id = 1:nsim,
                     mvtnorm::rmvnorm(n = nsim, mean = coef(lm_simple), sigma = vcov(lm_simple)))
colnames(boot_dat) <- c("id","intercept", "slope")
out_type <- knitr::opts_knit$get("rmarkdown.pandoc.to")
if(out_type == 'html'){
   g1 <- ggplot(data = boot_dat) +
    geom_point(data = SKD23_S2A, aes(x=proportion, y=pef), col = "grey", alpha = 0.8,
               position = position_jitter(width = 0.1, height = 0)) +
    geom_ribbon(data = predpi, aes(x = proportion, ymin = lwr, ymax = upr), fill = "grey70", alpha = 0.2) +
    geom_ribbon(data = predci, aes(x = proportion, ymin = lwr, ymax = upr), fill = "grey70", alpha = 0.5) +
    geom_abline(slope = coef(lm_simple)[2], intercept = coef(lm_simple)[1]) +
    scale_y_continuous(limits = c(1,7), oob = scales::squish, breaks = 1:7, labels = 1:7) +
    scale_x_continuous(limits = c(0,2), oob = scales::squish) +
    labs(x = "proportion of paper/plastic", subtitle = "perceived environmental friendliness",
         y = "") +
    geom_abline(mapping = aes(slope = slope, intercept = intercept), col = "gray10",
                linetype = "dashed", alpha = 0.4) + transition_manual(frames = id, cumulative = TRUE)
  g1
  g1
} else {
  ggplot(data = boot_dat) +
    geom_point(data = SKD23_S2A, aes(x=proportion, y=pef), col = "grey", alpha = 0.8,
               position = position_jitter(width = 0.1, height = 0)) +
    geom_ribbon(data = predpi, aes(x = proportion, ymin = lwr, ymax = upr), fill = "grey70", alpha = 0.2) +
    geom_ribbon(data = predci, aes(x = proportion, ymin = lwr, ymax = upr), fill = "grey70", alpha = 0.5) +
    geom_abline(slope = coef(lm_simple)[2], intercept = coef(lm_simple)[1]) +
    scale_y_continuous(limits = c(1,7), oob = scales::squish, breaks = 1:7, labels = 1:7) +
    scale_x_continuous(limits = c(0,2), oob = scales::squish) +
    labs(x = "proportion of paper/plastic", subtitle = "perceived environmental friendliness",
         y = "") +
    geom_abline(mapping = aes(slope = slope, intercept = intercept), col = "gray10",
                linetype = "dashed", alpha = 0.4)
}
```

## Width of intervals


```{r}
#| label: tbl-predints-soko
#| eval: true
#| echo: false
#| layout-ncol: 2
#| tbl-align: 'center'
#| tbl-cap: "Predictions with prediction intervals (left) and confidence intervals for the mean (right)."
data(SKD23_S2A, package = "hecedsm") # load data
lm_simple <- lm(pef ~ proportion, data = SKD23_S2A) # fit simple linear regression
tab1 <- predict(lm_simple,
        newdata = data.frame(proportion = c(0, 0.5, 1, 2)),
        interval = "prediction") # prediction intervals
tab2 <- predict(lm_simple,
        newdata = data.frame(proportion = c(0, 0.5, 1, 2)),
        interval = "confidence") # confidence for mean
knitr::kable(cbind(proportion = c(0, 0.5, 1, 2), tab1), align = c("cccc"),
             col.names = c("`proportion`", "prediction", "lower","upper"))
knitr::kable(tab2,
             col.names = c("mean", "lower CI","upper CI"),
             booktabs = TRUE, align = c("ccc"))
```

## Predictions in **R**

The generic `predict` takes as input

- a model and
- a `newdata` argument containing a data frame with the same structure as the original data
- a `type`, indicating the scale (`"response"` for linear models).
- an `interval`, either `"prediction"` or `"confidence"`, for objects of class `lm`.

```{r}
#| eval: false
#| echo: true
data(SKD23_S2A, package = "hecedsm") # load data
lm_simple <- lm(pef ~ proportion, data = SKD23_S2A) # fit simple linear regression
predict(lm_simple,
        newdata = data.frame(proportion = c(0, 0.5, 1, 2)),
        interval = "prediction") # prediction intervals
predict(lm_simple,
        newdata = data.frame(proportion = c(0, 0.5, 1, 2)),
        interval = "confidence") # confidence for mean
```

## References

